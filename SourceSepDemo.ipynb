{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh9l1P1uiAHI"
      },
      "outputs": [],
      "source": [
        "# requirements\n",
        "!pip install awscli;\n",
        "!pip install einops;\n",
        "!pip install museval;\n",
        "!pip install fast_bss_eval;\n",
        "!pip install torch_audiomentations;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "Please upload all the .py files from the project before proceeding.\n",
        "\n",
        "In order to test the entire dataset, run the commands to install musdb18hq.zip\n",
        "\n",
        "Load the variant(s) of the model you want to test using load_model and the checkpoints.\n",
        "\n",
        "All the necessary code is provided below.\n",
        "\n",
        "In order to test the file shown in the paper, download the mixture.wav and vocals.wav from the project repository."
      ],
      "metadata": {
        "id": "eQ8XMGNiMRfZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wblFvJzNii1s"
      },
      "outputs": [],
      "source": [
        "# import needed modules and functions\n",
        "import torch\n",
        "import torchaudio\n",
        "from IPython.display import Audio\n",
        "import tqdm.notebook as tq\n",
        "from model import Model, ChromaModel\n",
        "from trainer import load_model\n",
        "from trainer import hparams_def, hparams_chroma\n",
        "from eval import eval_dir, one_song_from_filepath\n",
        "import numpy as np\n",
        "from google.colab import drive, files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip musdb18hq.zip"
      ],
      "metadata": {
        "id": "MHuyjg4H8arf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9H_HARkEo--"
      },
      "outputs": [],
      "source": [
        "!aws configure set aws_access_key_id\n",
        "!aws configure set aws_secret_access_key\n",
        "!aws configure set default.region us-east-2\n",
        "!aws s3 cp s3://mymusicdatasets/chroma_attention_checkpoint_latest.pt ./\n",
        "!aws s3 cp s3://mymusicdatasets/chroma_fc_group_checkpoint_latest.pt ./\n",
        "!aws s3 cp s3://mymusicdatasets/checkpt_latest.pt ./\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to install the dataset !!!!! 27GB !!!!! it might not show progress as it downloads.\n",
        "!aws s3 cp s3://mymusicdatasets/musdb18hq.zip ./\n",
        "!unzip musdb18hq.zip"
      ],
      "metadata": {
        "id": "PigCIT_lL9SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRYt7zVEslqW"
      },
      "outputs": [],
      "source": [
        "# load bsrnn model - load in separate notebook to load bsrnn vs chroma models since the file names are the same and Colab does weird things with imports\n",
        "model = Model(hparams_def).eval()\n",
        "load_model('checkpt_latest.pt', model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load chroma attention bsrnn\n",
        "chroma_attention = ChromaModel(hparams_chroma, 'attention').eval()\n",
        "load_model('chroma_attention_checkpoint_latest.pt', chroma_attention)"
      ],
      "metadata": {
        "id": "l8Z4EwOk2XH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_fc = ChromaModel(hparams_chroma, 'group_fc').eval()\n",
        "load_model('chroma_fc_group_checkpoint_latest.pt', chroma_fc)"
      ],
      "metadata": {
        "id": "-QKDmdny4TCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGuZjOtoEoFg"
      },
      "outputs": [],
      "source": [
        "start_in_seconds = 45\n",
        "length_in_seconds = 45\n",
        "filepath = '/content/test/Mu - Too Bright/mixture.wav'\n",
        "source_path = '/content/test/Mu - Too Bright/vocals.wav'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "    axs.set_title(title or 'Spectrogram (db)')\n",
        "    axs.set_ylabel(ylabel)\n",
        "    axs.set_xlabel('frame')\n",
        "    im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
        "    if xmax:\n",
        "      axs.set_xlim((0, xmax))\n",
        "    fig.colorbar(im, ax=axs)\n",
        "    plt.show(block=False)"
      ],
      "metadata": {
        "id": "Q5zqYPf2BRAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import os\n",
        "import fast_bss_eval\n",
        "import museval\n",
        "import tqdm.notebook as tq\n",
        "def zero_pad(signal, segment_samples = 44100 * 6):\n",
        "    # assumption: even number of samples in a segment\n",
        "    hop_length = segment_samples // 2\n",
        "    if signal.shape[1] % hop_length != 0:\n",
        "        num_zeros = hop_length - (signal.shape[1] % hop_length)\n",
        "        zero_pad = torch.zeros(signal.shape[0], num_zeros, device = signal.device)\n",
        "        signal = torch.cat([signal, zero_pad], dim = 1)\n",
        "    return signal\n",
        "\n",
        "def split_to_segments(signal, segment_samples = 44100 * 6, overlap = 0.5):\n",
        "    # input shape: (#channel, samples)\n",
        "    # output shape: (#channels, #segments, segment_samples)\n",
        "    start, end = 0, segment_samples\n",
        "    segments = []\n",
        "    hop_length = int(segment_samples * (1 - overlap))\n",
        "    while end <= signal.shape[1]:\n",
        "        segment = signal[:, start:end]\n",
        "        start = start + hop_length\n",
        "        end = end + hop_length\n",
        "        segments.append(segment)\n",
        "    return torch.stack(segments, dim = 1)\n",
        "\n",
        "def predict_and_overlap(model, full_signal, segment_samples = 6 * 44100, overlap = 0.5, n_fft = 2048, hop_length = 512, win_length = 2048, show_progress = True):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = model.eval().to(device)\n",
        "    full_signal = full_signal.to(device)\n",
        "    length = full_signal.shape[1]\n",
        "    full_signal = zero_pad(full_signal)\n",
        "    segments = split_to_segments(full_signal)\n",
        "    num_channels, num_segments, segment_samples = segments.shape\n",
        "    output = torch.zeros_like(full_signal)\n",
        "    hop_samples = int(segment_samples * (1 - overlap))\n",
        "    start, end = 0, segment_samples\n",
        "    with torch.no_grad():\n",
        "        with tq.trange(num_segments, desc=\"Segment \") as segments_tq:\n",
        "            for num_segment in segments_tq:\n",
        "                input_segment = segments[:,num_segment,:].unsqueeze(0)\n",
        "                mask_prediction, input_spectrogram = model(input_segment)\n",
        "                stft_prediction = (mask_prediction * input_spectrogram)\n",
        "                stft_prediction = torch.complex(stft_prediction[0,:,:,:,0], stft_prediction[0,:,:,:,1])\n",
        "                signal_prediction = torch.istft(stft_prediction, n_fft = n_fft, hop_length = hop_length, win_length = win_length, length = segment_samples)\n",
        "                output[:,start:end] += signal_prediction\n",
        "                start += hop_samples\n",
        "                end += hop_samples\n",
        "    return output, full_signal\n",
        "\n",
        "def eval_dir(model, dir_path = 'test/', out_path = 'outputs/', full_test_mode = False):\n",
        "    filenames = os.listdir(dir_path)\n",
        "    cSDRs = []\n",
        "    for filename in filenames:\n",
        "        print(\"processing \" + filename)\n",
        "        mixture, sr1 = torchaudio.load(dir_path + filename + '/mixture.wav')\n",
        "        source, sr2 = torchaudio.load(dir_path + filename + '/vocals.wav')\n",
        "        assert(sr1 == sr2 and sr1 == 44100)\n",
        "        estimate = predict_and_overlap(model, mixture)\n",
        "        torchaudio.save(out_path + filename + '_vocal.wav', estimate.cpu().detach(), 44100, channels_first = True,)\n",
        "        if full_test_mode:\n",
        "            sdr, isr, sir, sar  = museval.evaluate(source.unsqueeze(0).permute(0, 2, 1).cpu().numpy(), estimate.unsqueeze(0).permute(0, 2, 1).cpu().numpy())\n",
        "        cSDR = np.mean(np.nanmedian(sdr, axis = 1))\n",
        "        print(\"cSDR: \", cSDR)\n",
        "        cSDRs.append(cSDR)\n",
        "    return cSDRs\n",
        "\n",
        "\n",
        "def one_song_from_filepath(filepath, model, source_path = \"\", offset = 0.0, length = -1, sampling_rate = 44100, force_mono = True, eval = False, full_eval_mode = False, plot_spectrograms = False):\n",
        "    signal, sr = torchaudio.load(filepath, frame_offset = int(offset * sampling_rate), num_frames = int(length * sampling_rate) if length != -1 else -1)\n",
        "    scores = []\n",
        "    if force_mono:\n",
        "        signal = torch.mean(signal, dim = 0).unsqueeze(0)\n",
        "    estimate, signal = predict_and_overlap(model, signal)\n",
        "    torchaudio.save(filepath[:-3] + '_vocals_pred.wav', estimate.cpu().detach(), 44100, channels_first = True,)\n",
        "    if eval and source_path != \"\":\n",
        "        source, sr = torchaudio.load(source_path, frame_offset = int(offset * sampling_rate), num_frames = int(length * sampling_rate) if length != -1 else -1)\n",
        "        if force_mono:\n",
        "            source = torch.mean(source, dim = 0).unsqueeze(0)\n",
        "        source = zero_pad(source).to(signal.device)\n",
        "        ref_chunks = torch.stack(torch.chunk(source, source.shape[1] // sampling_rate, dim = 1), dim = 0)\n",
        "        est_chunks = torch.stack(torch.chunk(estimate, estimate.shape[1] // sampling_rate, dim = 1), dim = 0)\n",
        "        csdr = fast_bss_eval.sdr(ref_chunks, est_chunks, use_cg_iter = 20, clamp_db = 30, load_diag = 1e-5)\n",
        "        csdr = torch.nanmedian(csdr)\n",
        "        scores.append(csdr)\n",
        "        if full_eval_mode:\n",
        "            sdr, isr, sir, sar  = museval.evaluate(source.unsqueeze(0).permute(0, 2, 1).cpu().numpy(), estimate.unsqueeze(0).permute(0, 2, 1).cpu().numpy())\n",
        "            cSDR = np.mean(np.nanmedian(sdr, axis = 1))\n",
        "            scores.append(cSDR)\n",
        "        if plot_spectrograms:\n",
        "            mixture_mag_stft = torch.abs(torch.stft(signal.squeeze(0), n_fft = 2048, hop_length = 512, return_complex = True))\n",
        "            source_mag_stft = torch.abs(torch.stft(source.squeeze(0), n_fft = 2048, hop_length = 512, return_complex = True))\n",
        "            estimate_mag_stft = torch.abs(torch.stft(estimate.squeeze(0), n_fft = 2048, hop_length = 512, return_complex = True))\n",
        "            plot_spectrogram(mixture_mag_stft.cpu(), title = \"Mixture spectrogram (dB)\")\n",
        "            plot_spectrogram(source_mag_stft.cpu(), title = \"Source spectrogram (dB)\")\n",
        "            plot_spectrogram(estimate_mag_stft.cpu(), title = \"Predicted source spectrogram (dB)\")\n",
        "        return scores\n"
      ],
      "metadata": {
        "id": "NSfCmRxGDGV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xndFznixEnor"
      },
      "outputs": [],
      "source": [
        "scores = one_song_from_filepath(filepath, model, offset = start_in_seconds, source_path = source_path, length = length_in_seconds, eval = True, full_eval_mode=True, plot_spectrograms = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = one_song_from_filepath(filepath, chroma_attention, offset = start_in_seconds, source_path = source_path, length = length_in_seconds, eval = True, full_eval_mode=True, plot_spectrograms = True)\n"
      ],
      "metadata": {
        "id": "5d8_1WVtGtuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = one_song_from_filepath(filepath, chroma_fc, offset = start_in_seconds, source_path = source_path, length = length_in_seconds, eval = True, full_eval_mode=True, plot_spectrograms = True)\n"
      ],
      "metadata": {
        "id": "MSGBImkWHaXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}